{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import plotly\n",
    "import re\n",
    "from datetime import datetime\n",
    "import explainerdashboard\n",
    "from d3blocks import d3blocks\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import docx\n",
    "from collections.abc import Iterable\n",
    "from pattern.en import sentiment, subjectivity\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "sw = set(stopwords.words('english'))\n",
    "list(sw)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(string.punctuation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employee Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_records_df = pd.read_excel('EmployeeRecords.xlsx', sheet_name='Employee Records')\n",
    "employee_records_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For datetime64[ns] types, NaT represents missing values. (Not a valid time). \n",
    "\n",
    "While NaN is the default missing value marker for reasons of computational speed and convenience. In many cases, however, the Python `None` will arise and we wish to also consider that “missing” or “not available” or “NA”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_employee_records_df = pd.read_excel('EmployeeRecords.xlsx', sheet_name='Index')\n",
    "index_employee_records_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_records_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_records_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_records_df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missingness means the employee did not serve in a military branch.\n",
    "\n",
    "The missingness from passport means they did not own a passport."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Create Full Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_records_df['FullName'] = employee_records_df['FirstName'] + ' ' + employee_records_df['LastName']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employee Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-byte charcter encoding of the latin alphabet\n",
    "email_df = pd.read_csv('emailheaders.csv', encoding='cp1252')\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email_df.shape)\n",
    "print(email_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripEmail(email):\n",
    "    return re.sub(pattern=r\"\\br\\.|@[a-z][a-z.+]+.\", repl=\"\", string=email)\n",
    "\n",
    "def addSplit(email):\n",
    "    return re.sub(pattern=\"\\.\", repl=\" \", string=email)\n",
    "\n",
    "email_df['From'] = email_df['From'].apply(lambda x: stripEmail(x))\n",
    "email_df['From'] = email_df['From'].apply(lambda x: addSplit(x))\n",
    "email_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#todo add assert from employee verification check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripEmail_From(email):\n",
    "    return re.sub(pattern=r\"\\br\\.|@[a-z][a-z.+]+.\", repl=\"\", string=email)\n",
    "\n",
    "def addSplit(email):\n",
    "    return re.sub(pattern=\"\\.\", repl=\" \", string=email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M/D/YYYY\n",
    "email_df['Date'] = pd.to_datetime(email_df['Date'], errors='raise')\n",
    "email_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = None \n",
    "\n",
    "def clean(text, stopwords) -> str:\n",
    "    \"\"\"\n",
    "    Clean text sentence \n",
    "    Params: text: the string to clean\n",
    "    stopwords: a list of NLTK stopwords to remove from input row\n",
    "    Returns: cleaned sentence\n",
    "    \"\"\" \n",
    "    text = re.sub(r'<[^>?]*>', '', text)\n",
    "    text_list = text.split()\n",
    "    text_words = []\n",
    "    punctuation = set(string.punctuation)\n",
    "    \n",
    "    for word in text_list: \n",
    "        while len(word) > 0 and word[0] in punctuation: \n",
    "            word = word[1:]\n",
    "        \n",
    "        while len(word) > 0 and word[-1] in punctuation: \n",
    "            word = word[:-1]\n",
    "            \n",
    "        word = word.replace(',', '')\n",
    "        word = word.replace('\\\"', '')\n",
    "        word = word.replace('\\'', '')\n",
    "\n",
    "        if len(word) > 0 and \"/\" not in word: \n",
    "            if word.lower() not in stopwords: \n",
    "                text_words.append(word.lower())\n",
    "        cleaner_text = \" \".join(text_words)\n",
    "    return cleaner_text\n",
    "\n",
    "email_df['Subject'] = email_df['Subject'].apply(clean, stopwords=sw)\n",
    "email_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News articles contain historical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_name(name) -> str:\n",
    "    \"\"\"\n",
    "    Params: str: article name\n",
    "    Returns:\n",
    "        str: shorten file name\n",
    "    \"\"\"\n",
    "    return \"article \" + re.sub(pattern=r\"\\.+.*\", repl=\"\", string=name)\n",
    "\n",
    "def extract_datetime(file) -> list:\n",
    "    \"\"\"\n",
    "    Extract datetime of news article for timeline\n",
    "    Params: article to parse\n",
    "    Returns: list of matches or empty list if no matches\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\d+/\\d+/\\d+', file) or re.findall(r'\\d+ \\w+ \\d+', file)\n",
    "\n",
    "def extract_data(rootdir, news_article) -> dict:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        rootdir (Path): path to search for article files\n",
    "        news_article (dict): empty dict to fill of <article, description>\n",
    "    Returns:\n",
    "        news_article: full <article, description> dict\n",
    "    \"\"\"\n",
    "    assert os.path.exists(rootdir)\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        assert os.path.exists(subdir)\n",
    "        for file in files:\n",
    "            if file.__contains__('txt'):\n",
    "                nav_file = os.path.join(subdir, file)\n",
    "                with open(nav_file, 'r') as datafile:\n",
    "                    try:\n",
    "                        news_name = extract_article_name(file)\n",
    "                        news_article[news_name] = datafile.read()\n",
    "                        datafile.close()\n",
    "                    except Exception as ex:\n",
    "                        print(\"Failed to parse article: \", ex)\n",
    "                        continue\n",
    "    return news_article\n",
    "\n",
    "news_article = {}\n",
    "rootdir = 'articles/'\n",
    "news_article = extract_data(rootdir, news_article)\n",
    "articles_df = pd.DataFrame.from_dict(data=news_article, orient='index', columns=['description'])\n",
    "articles_df['datetime'] = articles_df['description'].apply(lambda x: extract_datetime(x))\n",
    "articles_df.reset_index(inplace=True)\n",
    "print(articles_df.shape)\n",
    "print(articles_df.dtypes)\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['description'] = articles_df['description'].apply(clean, stopwords=sw)\n",
    "articles_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using resumes to look up historical employee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resume_name(name) -> str:\n",
    "    \"\"\"\n",
    "    Params: str: resume name\n",
    "    Returns:\n",
    "        str: shorten file name\n",
    "    \"\"\"\n",
    "    name = re.sub(pattern=r\"\\.+.*|Bio|Resume\", repl=\"\", string=name)\n",
    "    name = re.sub(pattern=r\"-\", repl=\" \", string=name)\n",
    "    return name\n",
    "\n",
    "def extract_data(rootdir, resumes) -> dict:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        rootdir (Path): path to search for article files\n",
    "        news_article (dict): empty dict to fill of <article, description>\n",
    "    Returns:\n",
    "        news_article: full <article, description> dict\n",
    "    \"\"\"\n",
    "    assert os.path.exists(rootdir)\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        assert os.path.exists(subdir)\n",
    "        for file in files:\n",
    "            if file.__contains__('docx'):\n",
    "                nav_file = os.path.join(subdir, file)\n",
    "                with open(nav_file, 'r') as datafile:\n",
    "                    try:\n",
    "                        resume_name = extract_resume_name(file)\n",
    "                        doc = docx.Document(nav_file)\n",
    "                        fullText = []\n",
    "                        for para in doc.paragraphs:\n",
    "                            fullText.append(para.text)\n",
    "                        resumes[resume_name] = '\\n'.join(fullText)\n",
    "                        datafile.close()\n",
    "                    except Exception as ex:\n",
    "                        print(\"Failed to parse resume: \", ex)\n",
    "                        continue\n",
    "    return resumes\n",
    "\n",
    "resumes = {}\n",
    "rootdir = 'resumes/'\n",
    "resumes = extract_data(rootdir, resumes)\n",
    "resumes_df = pd.DataFrame.from_dict(data=resumes, orient='index', columns=['resume'])\n",
    "resumes_df.reset_index(inplace=True)\n",
    "print(resumes_df.shape)\n",
    "resumes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_resume(text):\n",
    "    text = re.sub(r'<[^>?]*>', '', text)\n",
    "    text_list = text.split()\n",
    "    text_words = []\n",
    "    punctuation = set(string.punctuation)\n",
    "    \n",
    "    for word in text_list: \n",
    "        while len(word) > 0 and word[0] in punctuation: \n",
    "            word = word[1:]\n",
    "        \n",
    "        while len(word) > 0 and word[-1] in punctuation: \n",
    "            word = word[:-1]\n",
    "            \n",
    "        word = word.replace(',', '')\n",
    "        word = word.replace('\\\"', '')\n",
    "        word = word.replace('\\'', '')\n",
    "\n",
    "        if len(word) > 0 and \"/\" not in word: \n",
    "            text_words.append(word.lower())\n",
    "        cleaner_text = \" \".join(text_words)\n",
    "    return cleaner_text\n",
    "\n",
    "resumes_df['resume'] = resumes_df['resume'].apply(clean_text_resume)\n",
    "resumes_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_name(name) -> str:\n",
    "    \"\"\"\n",
    "    Params: str: resume name\n",
    "    Returns:\n",
    "        str: shorten file name\n",
    "    \"\"\"\n",
    "    return re.match(pattern=r\"^([0-9]+ year).*$\", string=name).group(1)\n",
    "\n",
    "def extract_data(rootdir, hist_doc) -> dict:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        rootdir (Path): path to search for files\n",
    "        news_article (dict): empty dict to fill of <article, description>\n",
    "    Returns:\n",
    "        news_article: full <article, description> dict\n",
    "    \"\"\"\n",
    "    assert os.path.exists(rootdir)\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        assert os.path.exists(subdir)\n",
    "        for file in files:\n",
    "            if file.__contains__('docx'):\n",
    "                nav_file = os.path.join(subdir, file)\n",
    "                with open(nav_file, 'r') as datafile:\n",
    "                    try:\n",
    "                        hist_name = extract_doc_name(file)\n",
    "                        doc = docx.Document(nav_file)\n",
    "                        fullText = []\n",
    "                        for para in doc.paragraphs:\n",
    "                            fullText.append(para.text)\n",
    "                        hist_doc[hist_name] = '\\n'.join(fullText)\n",
    "                        datafile.close()\n",
    "                    except Exception as ex:\n",
    "                        print(\"Failed to parse resume: \", ex)\n",
    "                        continue\n",
    "    return hist_doc\n",
    "\n",
    "historical_doc = {}\n",
    "rootdir = 'HistoricalDocuments/'\n",
    "historical_doc = extract_data(rootdir, historical_doc)\n",
    "historical_df = pd.DataFrame.from_dict(data=historical_doc, orient='index', columns=['description'])\n",
    "historical_df.reset_index(inplace=True)\n",
    "print(historical_df.shape)\n",
    "historical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df['description'].apply(clean_text_resume)\n",
    "historical_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_name(name) -> str:\n",
    "    \"\"\"\n",
    "    Params: str: resume name\n",
    "    Returns:\n",
    "        str: shorten file name\n",
    "    \"\"\"\n",
    "    return re.match(pattern=r\"^.*?(?=\\.)\", string=name).group(0)\n",
    "\n",
    "def extract_data(rootdir, hist_doc) -> dict:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        rootdir (Path): path to search for files\n",
    "        news_article (dict): empty dict to fill of <file name, description>\n",
    "    Returns:\n",
    "        news_article: full <file name, description> dict\n",
    "    \"\"\"\n",
    "    assert os.path.exists(rootdir)\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        assert os.path.exists(subdir)\n",
    "        for file in files:\n",
    "            if file.__contains__('docx'):\n",
    "                nav_file = os.path.join(subdir, file)\n",
    "                with open(nav_file, 'r') as datafile:\n",
    "                    try:\n",
    "                        print(file)\n",
    "                        file_name = str.lower(extract_doc_name(file))\n",
    "                        doc = docx.Document(nav_file)\n",
    "                        fullText = []\n",
    "                        for para in doc.paragraphs:\n",
    "                            fullText.append(para.text)\n",
    "                        factbook_doc[file_name] = '\\n'.join(fullText)\n",
    "                        datafile.close()\n",
    "                    except Exception as ex:\n",
    "                        print(\"Failed to parse factbook: \", ex)\n",
    "                        continue\n",
    "    return factbook_doc\n",
    "\n",
    "factbook_doc = {}\n",
    "rootdir = 'factbook/'\n",
    "factbook_doc = extract_data(rootdir, factbook_doc)\n",
    "factbook_df = pd.DataFrame.from_dict(data=factbook_doc, orient='index', columns=['description'])\n",
    "factbook_df.reset_index(inplace=True)\n",
    "print(factbook_df.shape)\n",
    "factbook_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factbook_df['description'].apply(clean_text_resume)\n",
    "factbook_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_name = email_df.copy()\n",
    "groupby_name = email_df.groupby(by=['From'])\n",
    "groupby_name = groupby_name.count().sort_values(by='To', ascending=False)\n",
    "groupby_name.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a commuication network within the GASTech organization and Protectors of Kronos members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_email_count = email_df.shape[0]\n",
    "groupby_name['Frequency'] = groupby_name['To'].apply(lambda x: (x/total_email_count)*100)\n",
    "groupby_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_network = email_df.groupby(by=['From'])\n",
    "comm_network.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How frequently do people contact each other? ie. Ada emailed Felix 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent = pd.DataFrame()\n",
    "for employee_from_name, employee_to_name in comm_network:\n",
    "    name_map_frequency = {}\n",
    "    for arr_name in employee_to_name['To']:\n",
    "        # check that email exists in employee records df\n",
    "        counter = 0\n",
    "        if arr_name.__contains__(\",\"):\n",
    "            arr_name = arr_name.split(\",\")\n",
    "            for employee in arr_name: \n",
    "                try:\n",
    "                    assert employee_records_df['EmailAddress'].eq(employee.strip()).any()\n",
    "                    counter+=1\n",
    "                    employee = stripEmail(employee)\n",
    "                    employee = addSplit(employee)\n",
    "                    name_map_frequency[employee] = name_map_frequency.get(employee, 0) + counter\n",
    "                except Exception as ex:\n",
    "                    print(employee)\n",
    "        else:\n",
    "            try:\n",
    "                assert employee_records_df['EmailAddress'].eq(arr_name).any()\n",
    "                counter+=1\n",
    "                arr_name = stripEmail(arr_name)\n",
    "                arr_name = addSplit(arr_name)\n",
    "                name_map_frequency[arr_name] = name_map_frequency.get(arr_name, 0) + counter\n",
    "            except Exception as ex:\n",
    "                print(arr_name)\n",
    "    emails_sent[employee_from_name] = name_map_frequency    \n",
    "emails_sent = emails_sent.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_df['To'] = email_df['To'].apply(lambda x: stripEmail_From(x))\n",
    "# email_df['To'] = email_df['To'].apply(lambda x: addSplit(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.columns = emails_sent.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.fillna(0, inplace=True)\n",
    "emails_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emails_sent.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent = emails_sent.loc[:,~emails_sent.columns.duplicated()].copy()\n",
    "emails_sent.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads row x column. ie. Ada sent Felix Resumir 2 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(emails_sent, x=emails_sent.index, y=emails_sent.columns.values, title='GASTech Employee Email Frequency Recipients')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have much insight other than Mat could be a bot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.columns = emails_sent.columns.str.strip()\n",
    "emails_sent.index = emails_sent.index.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent = emails_sent.unstack().reset_index().rename(columns={'level_0': 'target', 'level_1': 'source', 0: 'weight'})\n",
    "emails_sent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GASTech Communication Network Chord Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent.to_csv('emails_sent_header.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Felix Resumir</td>\n",
       "      <td>Ada Campo-Corrente</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target              source  weight\n",
       "0  Felix Resumir  Ada Campo-Corrente     2.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_sent.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d3blocks.D3Blocks()\n",
    "# d3.set_node_properties(emails_sent, color='source-target', opacity=100)\n",
    "d3.chord(df=emails_sent, color='source-target', opacity=100, title='GASTech Communication Network Chord Graph', filepath='./d3blocks.html', figsize=[1000,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\element\\graphs.py:642: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n",
      "c:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\element\\graphs.py:642: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m recipients \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(emails_sent[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mtolist() \u001b[39m+\u001b[39m emails_sent[\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mtolist()))\n\u001b[0;32m      5\u001b[0m email_HV_dataset \u001b[39m=\u001b[39m hv\u001b[39m.\u001b[39mDataset(pd\u001b[39m.\u001b[39mDataFrame(recipients, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mEmail\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m----> 9\u001b[0m chord \u001b[39m=\u001b[39m hv\u001b[39m.\u001b[39;49mChord(data\u001b[39m=\u001b[39;49m(emails_sent, email_HV_dataset))\n\u001b[0;32m     10\u001b[0m chord\u001b[39m.\u001b[39mopts(\n\u001b[0;32m     11\u001b[0m     opts\u001b[39m.\u001b[39mChord(cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCategory20\u001b[39m\u001b[39m'\u001b[39m, edge_cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCategory20\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[39m# chord.opts(\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m#     opts.Chord(cmap='Category20', edge_cmap='Category20', edge_color=dim('Email').str(), \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m#                labels='name', node_color=dim('index').str()))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\element\\graphs.py:764\u001b[0m, in \u001b[0;36mChord.__init__\u001b[1;34m(self, data, kdims, vdims, compute, **params)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[0;32m    763\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes \u001b[39m=\u001b[39m nodes\n\u001b[1;32m--> 764\u001b[0m     chord \u001b[39m=\u001b[39m layout_chords(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    765\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes \u001b[39m=\u001b[39m chord\u001b[39m.\u001b[39mnodes\n\u001b[0;32m    766\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edgepaths \u001b[39m=\u001b[39m chord\u001b[39m.\u001b[39medgepaths\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\param\\parameterized.py:3658\u001b[0m, in \u001b[0;36mParameterizedFunction.__new__\u001b[1;34m(class_, *args, **params)\u001b[0m\n\u001b[0;32m   3656\u001b[0m inst \u001b[39m=\u001b[39m class_\u001b[39m.\u001b[39minstance()\n\u001b[0;32m   3657\u001b[0m inst\u001b[39m.\u001b[39mparam\u001b[39m.\u001b[39m_set_name(class_\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m-> 3658\u001b[0m \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\core\\operation.py:220\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, element, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mlink_dataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_propagate_dataset\n\u001b[0;32m    219\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mlink_inputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp\u001b[39m.\u001b[39mlink_inputs\n\u001b[1;32m--> 220\u001b[0m \u001b[39mreturn\u001b[39;00m element\u001b[39m.\u001b[39mapply(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\core\\accessors.py:43\u001b[0m, in \u001b[0;36mAccessorPipelineMeta.pipelined.<locals>.pipelined_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     inst\u001b[39m.\u001b[39m_obj\u001b[39m.\u001b[39m_in_method \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_method:\n\u001b[0;32m     46\u001b[0m         init_op \u001b[39m=\u001b[39m factory\u001b[39m.\u001b[39minstance(\n\u001b[0;32m     47\u001b[0m             output_type\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(inst),\n\u001b[0;32m     48\u001b[0m             kwargs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mgetattr\u001b[39m(inst, \u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)},\n\u001b[0;32m     49\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\core\\accessors.py:199\u001b[0m, in \u001b[0;36mApply.__call__\u001b[1;34m(self, apply_function, streams, link_inputs, link_dataset, dynamic, per_element, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(apply_function, \u001b[39m'\u001b[39m\u001b[39mdynamic\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    198\u001b[0m     inner_kwargs[\u001b[39m'\u001b[39m\u001b[39mdynamic\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m new_obj \u001b[39m=\u001b[39m apply_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minner_kwargs)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m (link_dataset \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj, Dataset) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[39misinstance\u001b[39m(new_obj, Dataset) \u001b[39mand\u001b[39;00m new_obj\u001b[39m.\u001b[39m_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    202\u001b[0m     new_obj\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj\u001b[39m.\u001b[39mdataset\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\core\\operation.py:214\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, element, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[39mreturn\u001b[39;00m element\u001b[39m.\u001b[39mclone([(k, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(el, key\u001b[39m=\u001b[39mk))\n\u001b[0;32m    211\u001b[0m                               \u001b[39mfor\u001b[39;00m k, el \u001b[39min\u001b[39;00m element\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m    212\u001b[0m     \u001b[39melif\u001b[39;00m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_per_element \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(element, Element)) \u001b[39mor\u001b[39;00m\n\u001b[0;32m    213\u001b[0m           (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_per_element \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(element, ViewableElement))):\n\u001b[1;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(element)\n\u001b[0;32m    215\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mstreams\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    216\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstreams\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp\u001b[39m.\u001b[39mstreams\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\core\\operation.py:141\u001b[0m, in \u001b[0;36mOperation._apply\u001b[1;34m(self, element, key)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_method:\n\u001b[0;32m    140\u001b[0m         element\u001b[39m.\u001b[39m_in_method \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process(element, key)\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(element, \u001b[39m'\u001b[39m\u001b[39m_in_method\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m in_method:\n\u001b[0;32m    143\u001b[0m     element\u001b[39m.\u001b[39m_in_method \u001b[39m=\u001b[39m in_method\n",
      "File \u001b[1;32mc:\\Users\\mcmil\\UniversityofDenver\\COMP4449-capstone\\venvCOMP4449\\lib\\site-packages\\holoviews\\element\\graphs.py:673\u001b[0m, in \u001b[0;36mlayout_chords._process\u001b[1;34m(self, element, key)\u001b[0m\n\u001b[0;32m    671\u001b[0m n_conn \u001b[39m=\u001b[39m weights_of_areas[i]\n\u001b[0;32m    672\u001b[0m p0, p1 \u001b[39m=\u001b[39m points[i], points[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 673\u001b[0m angles \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(p0, p1, \u001b[39mint\u001b[39;49m(n_conn))\n\u001b[0;32m    674\u001b[0m coords \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(np\u001b[39m.\u001b[39mcos(angles), np\u001b[39m.\u001b[39msin(angles)))\n\u001b[0;32m    675\u001b[0m all_areas\u001b[39m.\u001b[39mappend(coords)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# import holoviews as hv\n",
    "# from holoviews import opts, dim\n",
    "\n",
    "# recipients = list(set(emails_sent[\"target\"].unique().tolist() + emails_sent[\"source\"].unique().tolist()))\n",
    "# email_HV_dataset = hv.Dataset(pd.DataFrame(recipients, columns=[\"Email\"]))\n",
    "\n",
    "# chord = hv.Chord(data=(emails_sent, email_HV_dataset))\n",
    "# chord.opts(\n",
    "#     opts.Chord(cmap='Category20', edge_cmap='Category20'))\n",
    "# chord.opts(\n",
    "#     opts.Chord(cmap='Category20', edge_cmap='Category20', edge_color=dim('Email').str(), \n",
    "#                labels='name', node_color=dim('index').str()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_birth = employee_records_df.groupby(by=['BirthCountry'], as_index=False).size()\n",
    "px.bar(grouped_birth, x='BirthCountry', y='size', title='Birth Country Count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asteria isn't mentioned anywhere in any documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_employment = employee_records_df.groupby(by=['CurrentEmploymentType'], as_index=False).size()\n",
    "px.bar(grouped_employment, x='CurrentEmploymentType', y='size', title='Current Department Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_title = employee_records_df.groupby(by=['CurrentEmploymentTitle'], as_index=False).size()\n",
    "px.bar(grouped_title, x='CurrentEmploymentTitle', y='size', title='Employee Title Count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query if a citzen's birth country is not the same as their current citizenship status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizen_transfer = employee_records_df.copy()\n",
    "citizen_transfer = citizen_transfer.query(expr=\"BirthCountry != CitizenshipCountry\")\n",
    "citizen_transfer.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Are there any symapthizers within GASTech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympathizers_kronos = email_df.query(\"Subject.str.contains('kronos')\", engine='python', inplace=False)\n",
    "print(sympathizers_kronos.shape)\n",
    "sympathizers_kronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympathizers = sympathizers_kronos.query(\"Subject.str.contains('defenders')\", engine='python', inplace=False)\n",
    "sympathizers = sympathizers[['From']]\n",
    "employee_sympatizers = employee_records_df.apply(lambda x: x[employee_records_df['FullName'].isin(sympathizers['From'])])\n",
    "employee_sympatizers.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruscella Mies Harver, Isia Vann, Loreto Bodrogi, and Inga Ferro are likely PoK symaptizers. Their citizenship is from Kronos, served in the miliatary as well as sending out email contents of defenders of Kronos propaganda. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of News Articles with Pattern Library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis with Pattern\n",
    "https://github.com/clips/pattern/wiki/pattern-en \n",
    "\n",
    "Written text can be broadly categorized into two types: facts and opinions. Opinions carry people's sentiments, appraisals and feelings toward the world. The pattern.en module bundles a lexicon of adjectives (e.g., good, bad, amazing, irritating, ...) that occur frequently in product reviews, annotated with scores for sentiment polarity (positive ↔ negative) and subjectivity (objective ↔ subjective). The sentiment() function returns a (polarity, subjectivity)-tuple for the given sentence, based on the adjectives it contains, where polarity is a value between -1.0 and +1.0 and subjectivity between 0.0 and 1.0. The sentence can be a string, Text, Sentence, Chunk, Word or a Synset (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(sentence) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform Sentiment analysis on recieved text\n",
    "    Params: str: sentence\n",
    "    Returns:\n",
    "        tuple: polarity(sentiment score), subjectivity(sentiment modality)\n",
    "         Polarity returns the average in the sentence.\n",
    "         Subjectivity quantifies the amount of personal opinion and factual information contained in the text. \n",
    "         The higher subjectivity means that the text contains personal opinion rather than factual information.\n",
    "    \"\"\"\n",
    "    sentiment_score, sentiment_modality = sentiment(sentence)\n",
    "    return sentiment_score, sentiment_modality\n",
    "\n",
    "articles_df['sentiment_score'], articles_df['sentiment_modality'] = zip(*articles_df['description'].apply(lambda x: sentiment_analysis(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment score a value between -1.0 - +1.0. Where -1 means negative sentiment, 0 neutral, and 1 is postive sentiment.\n",
    "\n",
    "Sentiment modality a value between 0 - 1.0. Where 0 is a fact and 1.0 is a person's opinion\n",
    "\n",
    "** This is sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(articles_df, x='index', y='sentiment_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(articles_df, x='index', y='sentiment_modality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_news = articles_df.query(\"description.str.contains('police')\", engine='python', inplace=False)\n",
    "print(police_news.shape)\n",
    "police_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_df = police_news.query(\"description.str.contains('blotter')\", engine='python', inplace=False)\n",
    "arrest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arrest_df.description.values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline of Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDatetime(date):\n",
    "    format = \"%Y/%m/%d\"\n",
    "    for date_string in date:\n",
    "        try:\n",
    "            date = datetime.strptime(date_string, format).date()\n",
    "        except Exception as ex: \n",
    "            print(\"Failed to parse datetime stamp: \", ex)\n",
    "    return date\n",
    "\n",
    "articles_df['datetime'] = articles_df['datetime'].apply(lambda x: parseDatetime(x)) \n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['datetime'] = pd.to_datetime(articles_df['datetime']).dt.date\n",
    "print(articles_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_arrested_articles(employee, arrest_df):\n",
    "#     if arrest_df['description'].__contains__(employee): \n",
    "#         return employee, True\n",
    "\n",
    "# arrest_df['GASTech Employee'], arrest_df['Arrested'] = zip(*employee_records_df['FullName'].apply(lambda e: search_arrested_articles(e, arrest_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chord_df = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
    "\n",
    "# tmp_lst = []\n",
    "\n",
    "# for employee_from in emails_sent.index:\n",
    "#     for employee_to in emails_sent.columns:\n",
    "#         # record = (employee_from, employee_to, emails_sent[employee_to][employee_from])\n",
    "#         record = (employee_from, employee_to, emails_sent[employee_to][employee_from])\n",
    "#         tmp_lst.append(record)\n",
    "\n",
    "# tmp = pd.DataFrame(tmp_lst)\n",
    "# tmp\n",
    "# chord_df = emails_sent.reset_index().apply(lambda e: [(e[1], e[0], emails_sent[e])])\n",
    "\n",
    "# chord_df= emails_sent.index.to_series().str.get(1).apply(lambda e: [(e[1], e[0], emails_sent[e])])\n",
    "\n",
    "# df2 = pd.MultiIndex.from_tuples(tmp_lst)\n",
    "# def split(x):\n",
    "#     return x[0][0], x[0][1], str(x[0][2])\n",
    "\n",
    "# df2 = pd.DataFrame()\n",
    "# df2['source'], df2['target'], df2['weight'] = tmp.map()\n",
    "\n",
    "# df2 = pd.DataFrame(emails_sent.unstack())\n",
    "# df2\n",
    "\n",
    "\n",
    "# chord_df['source'], chord_df['target'], chord_df['weight'] = zip(*emails_sent.index.apply(lambda e: [splitchord(e, employee_) for employee_ in emails_sent.columns], axis=1))\n",
    "# chord_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# def define_model(vocab_size, max_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "#     model.add(Conv1D(32, 8, activation='relu'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     # compile network\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     # summarize defined model\n",
    "#     model.summary()\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "# model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# model.save('model.h5')\n",
    "# train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "# # load the model\n",
    "# model = load_model('model.h5')\n",
    "# # evaluate model on training dataset\n",
    "# _, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "# print('Train Accuracy: %f' % (acc*100))\n",
    "# # evaluate model on test dataset\n",
    "# _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvcomp4449",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
